{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NithyaPKiran/Assignments/blob/main/Assignment_11_Text_Mining_1_Elon_Musk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PBRePoHJ12yr"
      },
      "cell_type": "markdown",
      "source": [
        "# TEXT MINING"
      ]
    },
    {
      "metadata": {
        "id": "k197S70ywY3b"
      },
      "cell_type": "markdown",
      "source": [
        "PROBLEM STATEMENT:\n",
        "\n",
        "Perform sentimental analysis on the Elon-musk tweets (Elon-musk.csv)"
      ]
    },
    {
      "metadata": {
        "id": "K-5fiIVo2ETW",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Import the Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvlCAks82Yaz",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-31T14:51:20.140751Z",
          "start_time": "2023-05-31T14:51:19.904641Z"
        },
        "id": "uFgf0e2R2ptH",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Read the file\n",
        "data = pd.read_csv('Elon_musk.csv',encoding=\"latin-1\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BlCcQyw65qEZ",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Number of Words in single tweet\n",
        "data['word_count'] = data['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "data[['Text','word_count']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M6MKSBUO5zA3",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Number of characters in single tweet\n",
        "data['char_count'] = data['Text'].str.len() ## this also includes spaces\n",
        "data[['Text','char_count']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Af-ShCa52Fh",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Average Word Length\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "data['avg_word'] = data['Text'].apply(lambda x: avg_word(x))\n",
        "data[['Text','avg_word']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "13v3jzOB55MS",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Number of stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data['stopwords'] = data['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "data[['Text','stopwords']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVOYxcDq58rw",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Number of Special Characters\n",
        "data['hastags'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
        "data[['Text','hastags']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_avj1nOD6q1X",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Number of Numerics\n",
        "data['numerics'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "data[['Text','numerics']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIpkdQMv6tLo",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Number of Upper Case Words\n",
        "data['upper'] = data['Text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "data[['Text','upper']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNdqygV46wH_",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Lower Case\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeNvMM1Y6zAj",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Removing Punctuation\n",
        "data['Text'] = data['Text'].str.replace('[^\\w\\s]','')\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rgSpw_MT63Tl",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Removal of Stop Words\n",
        "stop = stopwords.words('english')\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1V8_7bKE66PY",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Common word removal\n",
        "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[:10]\n",
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dEeVFq0E68rK",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "freq = list(freq.index)\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rr8xH2XS7CVu",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Rare Words Removal\n",
        "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[-10:]\n",
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmuDZYaS7Gc_",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "freq = list(freq.index)\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKcybf4o7KHW",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "# Spelling correction\n",
        "data['Text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wz7Ly5iz7OQg",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fChni_oV7Qlg",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "TextBlob(data['Text'][1]).words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqbKpDzN8DmM",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3SVKe6h8GgQ",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "data['Text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWwj6k4g8LMa",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "from textblob import Word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zTQ9LuY8NJH",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "data['Text'] = data['Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "data['Text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8AYEqNr8PFv",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#N-grams\n",
        "TextBlob(data['Text'][0]).ngrams(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GPGu34d8TI3",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Term Frequency\n",
        "tf1 = (data['Text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "tf1.columns = ['words','tf']\n",
        "tf1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tiU4Zu2T8YA3",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Inverse document frequency\n",
        "for i,word in enumerate(tf1['words']):\n",
        "  tf1.loc[i, 'idf'] = np.log(data.shape[0]/(len(data[data['Text'].str.contains(word)])))\n",
        "\n",
        "tf1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whWlT4Sq8el-",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Term Frequency â€“ Inverse Document Frequency (TF-IDF)\n",
        "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
        "tf1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJ1BdIOQ8haf",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
        " stop_words= 'english',ngram_range=(1,1))\n",
        "vect = tfidf.fit_transform(data['Text'])\n",
        "vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ph2khIu88jnz",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
        "data_bow = bow.fit_transform(data['Text'])\n",
        "data_bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8iHqikWr8n9C",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "#Sentiment Analysis\n",
        "data['Text'][:5].apply(lambda x: TextBlob(x).sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-hKWhcL8r67",
        "trusted": false
      },
      "cell_type": "code",
      "source": [
        "data['sentiment'] = data['Text'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
        "data[['Text','sentiment']].head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "gist": {
      "id": "",
      "data": {
        "description": "TEXT_MINING(ELON_MUSK).ipynb",
        "public": true
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}